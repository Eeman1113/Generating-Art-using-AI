{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generating art using CC12M.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhusQLqZ8F1iGwdHwadTWv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eeman1113/Generating-Art-using-AI/blob/main/Generating_art_using_CC12M.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0KEFE7ENuV4"
      },
      "outputs": [],
      "source": [
        "# Check the GPU\n",
        "\n",
        "!mkdir steps\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "\n",
        "!pip install ftfy regex requests tqdm\n",
        "!git clone --recursive https://github.com/crowsonkb/v-diffusion-pytorch"
      ],
      "metadata": {
        "id": "j7JXwsnDN1A9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the diffusion model\n",
        "# SHA-256: 4fc95ee1b3205a3f7422a07746383776e1dbc367eaf06a5b658ad351e77b7bda\n",
        "\n",
        "!mkdir v-diffusion-pytorch/checkpoints\n",
        "!curl -L https://v-diffusion.s3.us-west-2.amazonaws.com/cc12m_1_cfg.pth > v-diffusion-pytorch/checkpoints/cc12m_1_cfg.pth"
      ],
      "metadata": {
        "id": "BQLoVtUBN6VB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "import gc\n",
        "import math\n",
        "import sys\n",
        "\n",
        "from IPython import display\n",
        "import torch\n",
        "from torchvision import utils as tv_utils\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "sys.path.append('/content/v-diffusion-pytorch')\n",
        "\n",
        "from CLIP import clip\n",
        "from diffusion import get_model, sampling, utils"
      ],
      "metadata": {
        "id": "QiDL7BKsN9KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q requests tqdm ftfy\n",
        "\n",
        "%cd /content\n",
        "!git clone --recursive https://github.com/crowsonkb/v-diffusion-pytorch.git\n",
        "\n",
        "%cd /content/v-diffusion-pytorch\n",
        "%mkdir -p checkpoints\n",
        "!curl https://v-diffusion.s3.us-west-2.amazonaws.com/cc12m_1_cfg.pth -o checkpoints/cc12m_1_cfg.pth\n",
        "\n",
        "!./cfg_sample.py \"the rise of consciousness\":5 -n 4 -bs 4 --seed 0"
      ],
      "metadata": {
        "id": "I5wTfzeTRbaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the models\n",
        "\n",
        "model = get_model('cc12m_1_cfg')()\n",
        "_, side_y, side_x = model.shape\n",
        "model.load_state_dict(torch.load('/content/v-diffusion-pytorch/checkpoints/cc12m_1_cfg.pth', map_location='cpu'))\n",
        "model = model.half().cuda().eval().requires_grad_(False)\n",
        "clip_model = clip.load(model.clip_model, jit=False, device='cpu')[0]"
      ],
      "metadata": {
        "id": "dVaNHfhbN93p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# side_y = 512\n",
        "# side_x = 512"
      ],
      "metadata": {
        "id": "QgVO-cWiOA0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Only these heights and widths work: 4,8,16,32,64,128,256,512"
      ],
      "metadata": {
        "id": "turj0LGVrYej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4,8,16,32,64,128,256,512\n",
        "#code to generate valid hieght and width\n",
        "a=4\n",
        "b=1\n",
        "c=0\n",
        "while b<=2000:\n",
        "  c=a*2\n",
        "  print(c,end=\",\")\n",
        "  a=c\n",
        "  b+=1\n"
      ],
      "metadata": {
        "id": "ad21KcIzt0jR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Only these heights and widths work: 4,8,16,32,64,128,256,512\n",
        "#@title Settings\n",
        "import random\n",
        "prompt = 'Fantastical Landscape of the Imagination, Concept Art'  #@param {type:\"string\"}\n",
        "\n",
        "height =  512#@param {type:\"integer\"}\n",
        "width =  512#@param {type:\"integer\"}\n",
        "side_x = width\n",
        "side_y = height\n",
        "#@markdown Specify the number of diffusion timesteps (default is 50, can lower for faster but lower quality sampling).\n",
        "steps =   2000#@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "#@markdown Sample this many images.\n",
        "n_images =   1#@param {type:\"integer\"}\n",
        "\n",
        "weight = 5 \n",
        "\n",
        "#@markdown Set to 0 for deterministic (DDIM) sampling, 1 (the default) for stochastic (DDPM) sampling, and in between to interpolate between the two. 0 is preferred for low numbers of timesteps.\n",
        "eta =   0#@param {type:\"number\"}\n",
        "\n",
        "#@markdown The random seed. Change this to sample different images. -1 means completely random seed!\n",
        "seed =   -1#@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Display progress every this many timesteps.\n",
        "display_every =   5#@param {type:\"integer\"}\n",
        "\n",
        "save_progress_video = True #@param {type:\"boolean\"}"
      ],
      "metadata": {
        "id": "FPt_gfgyODLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run the Generator\n",
        "from PIL import ImageFile, Image\n",
        "import numpy as np\n",
        "import os\n",
        "target_embed = clip_model.encode_text(clip.tokenize(prompt)).float().cuda()\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def cfg_model_fn(x, t):\n",
        "    \"\"\"The CFG wrapper function.\"\"\"\n",
        "    n = x.shape[0]\n",
        "    x_in = x.repeat([2, 1, 1, 1])\n",
        "    t_in = t.repeat([2])\n",
        "    clip_embed_repeat = target_embed.repeat([n, 1])\n",
        "    clip_embed_in = torch.cat([torch.zeros_like(clip_embed_repeat), clip_embed_repeat])\n",
        "    v_uncond, v_cond = model(x_in, t_in, clip_embed_in).chunk(2, dim=0)\n",
        "    v = v_uncond + (v_cond - v_uncond) * weight\n",
        "    return v\n",
        "save_name = 0.00000000\n",
        "\n",
        "def display_callback(info):\n",
        "    global save_name\n",
        "    save_name += 0.00000001\n",
        "    nrow = math.ceil(info['pred'].shape[0]**0.5)\n",
        "    grid = tv_utils.make_grid(info['pred'], nrow, padding=0)\n",
        "    utils.to_pil_image(grid).save(f\"/content/steps/%.8f.png\" % save_name)\n",
        "    \n",
        "    if info['i'] % display_every == 0:\n",
        "        nrow = math.ceil(info['pred'].shape[0]**0.5)\n",
        "        grid = tv_utils.make_grid(info['pred'], nrow, padding=0)\n",
        "        tqdm.write(f'Step {info[\"i\"]} of {steps}:')\n",
        "        display.display(utils.to_pil_image(grid))\n",
        "        # utils.to_pil_image(grid).save(\"./steps/Test.png\")\n",
        "        \n",
        "        tqdm.write(f'')\n",
        "\n",
        "\n",
        "def run(seed):\n",
        "    print(\"Prompt is: \" + prompt)\n",
        "    \n",
        "    if seed == -1:\n",
        "       seed = random.randint(0, 2**32)\n",
        "      # print(seed)\n",
        "    print(\"Seed is: \" + str(seed))\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.manual_seed(seed)\n",
        "    x = torch.randn([n_images, 3, side_y, side_x], device='cuda')\n",
        "    t = torch.linspace(1, 0, steps + 1, device='cuda')[:-1]\n",
        "    step_list = utils.get_spliced_ddpm_cosine_schedule(t)\n",
        "    outs = sampling.sample(cfg_model_fn, x, step_list, eta, {}, callback=display_callback)\n",
        "    tqdm.write('Done!')\n",
        "    for i, out in enumerate(outs):\n",
        "        filename = f'out_{i}.png'\n",
        "        utils.to_pil_image(out).save(filename)\n",
        "        display.display(display.Image(filename))\n",
        "\n",
        "\n",
        "run(seed)\n",
        "frames = []\n",
        "files = []\n",
        "init_frame = 1 #Este es el frame donde el vídeo empezará\n",
        "last_frame = steps #Puedes cambiar i a el número del último frame que quieres generar. It will raise an error if that number of frames does not exist.\n",
        "\n",
        "directory = '/content/steps/'\n",
        "    # iterate over files in\n",
        "    # that directory\n",
        "for filename in os.listdir(directory):\n",
        "    f = os.path.join(directory, filename)\n",
        "        # checking if it is a file\n",
        "    files.append(f)\n",
        "for i in range(init_frame,last_frame): #\n",
        "    # save_name2 += 0.00000001\n",
        "    # filename = f\"steps/{save_name2}.png\"\n",
        "    # print(files[i])\n",
        "    frames.append(Image.open(files[i]))\n",
        "frames[-1].save(\"finalgrid.png\")\n",
        "\n",
        "\n",
        "if save_progress_video:\n",
        "\n",
        "    init_frame = 1 #Este es el frame donde el vídeo empezará\n",
        "    last_frame = steps #Puedes cambiar i a el número del último frame que quieres generar. It will raise an error if that number of frames does not exist.\n",
        "\n",
        "    min_fps = 10\n",
        "    max_fps = 30\n",
        "\n",
        "    total_frames = last_frame-init_frame\n",
        "\n",
        "    length = 15 #Tiempo deseado del vídeo en segundos\n",
        "\n",
        "    # import required module\n",
        "    import os\n",
        "    # assign directory\n",
        "    directory = '/content/steps/'\n",
        "    \n",
        "    files = []\n",
        "\n",
        "    # iterate over files in\n",
        "    # that directory\n",
        "    for filename in os.listdir(directory):\n",
        "        f = os.path.join(directory, filename)\n",
        "        # checking if it is a file\n",
        "        files.append(f)\n",
        "\n",
        "\n",
        "    files.sort()\n",
        "    # print(files)\n",
        "\n",
        "\n",
        "    frames = []\n",
        "    save_name2 = 0.00000000\n",
        "    tqdm.write('Generating video...')\n",
        "    for i in range(init_frame,last_frame): #\n",
        "        # save_name2 += 0.00000001\n",
        "        # filename = f\"steps/{save_name2}.png\"\n",
        "        # print(files[i])\n",
        "        frames.append(Image.open(files[i]))\n",
        "    # frames[-1].save(\"finalgrid.png\")\n",
        "    #fps = last_frame/10\n",
        "    fps = np.clip(total_frames/length,min_fps,max_fps)\n",
        "\n",
        "    from subprocess import Popen, PIPE\n",
        "    p = Popen(['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'png', '-r', str(fps), '-i', '-', '-vcodec', 'libx264', '-r', str(fps), '-pix_fmt', 'yuv420p', '-crf', '17', '-preset', 'veryslow', 'video.mp4'], stdin=PIPE)\n",
        "    for im in tqdm(frames):\n",
        "        im.save(p.stdin, 'PNG')\n",
        "    p.stdin.close()\n",
        "\n",
        "    p.wait()\n",
        "    print(\"Video finished Rendering\")"
      ],
      "metadata": {
        "id": "oZCd5Va-OK_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Show Generated Video in Colab\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "display.HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "metadata": {
        "id": "TS1I6uB6OTWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#end "
      ],
      "metadata": {
        "id": "_nW57Nd3OXdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "JS to prevent idle timeout:\n",
        "\n",
        "Press F12 OR CTRL + SHIFT + I OR right click  -> inspect.\n",
        "Then click on the console tab and paste in the following code.\n",
        "\n",
        "```javascript\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "```"
      ],
      "metadata": {
        "id": "1nDIlGiuncwC"
      }
    }
  ]
}